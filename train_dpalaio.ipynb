{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_u6rm6B6LR2",
        "outputId": "6c55dbe4-12eb-419c-eb81-667d311309d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[box2d]~=0.18.0\n",
            "  Downloading gym-0.18.3.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 15.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]~=0.18.0) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]~=0.18.0) (1.21.6)\n",
            "Collecting pyglet<=1.5.15,>=1.4.0\n",
            "  Downloading pyglet-1.5.15-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow<=8.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]~=0.18.0) (7.1.2)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]~=0.18.0) (1.5.0)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 69.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.18.3-py3-none-any.whl size=1657535 sha256=2a10d2a7eac192b08cac15b39f5aa43bdc5f9fafb0649088b85f89e92f86e98e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/ec/6d/705d53925f481ab70fd48ec7728558745eeae14dfda3b49c99\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym, box2d-py\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed box2d-py-2.3.8 gym-0.18.3 pyglet-1.5.15\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[box2d]~=0.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGHTVuGR6PvI",
        "outputId": "3bd1b996-8faf-4074-c47c-2fdb00276946"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.2\n",
            "pygame 2.1.2 (SDL 2.0.16, Python 3.7.15)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Surface(640x480x32 SW)>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install pygame \n",
        "\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame\n",
        "pygame.display.set_mode((640,480))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U98nCPGRDtCV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size, input_shape):\n",
        "        self.size = size\n",
        "        self.counter = 0\n",
        "        self.state_buffer = np.zeros((self.size, input_shape), dtype=np.float32)\n",
        "        self.action_buffer = np.zeros(self.size, dtype=np.int32)\n",
        "        self.reward_buffer = np.zeros(self.size, dtype=np.float32)\n",
        "        self.new_state_buffer = np.zeros((self.size, input_shape), dtype=np.float32)\n",
        "        self.terminal_buffer = np.zeros(self.size, dtype=np.bool_)\n",
        "\n",
        "    def store_tuples(self, state, action, reward, new_state, done):\n",
        "        idx = self.counter % self.size\n",
        "        self.state_buffer[idx] = state\n",
        "        self.action_buffer[idx] = action\n",
        "        self.reward_buffer[idx] = reward\n",
        "        self.new_state_buffer[idx] = new_state\n",
        "        self.terminal_buffer[idx] = done\n",
        "        self.counter += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_buffer = min(self.counter, self.size)\n",
        "        batch = np.random.choice(max_buffer, batch_size, replace=False)\n",
        "        state_batch = self.state_buffer[batch]\n",
        "        action_batch = self.action_buffer[batch]\n",
        "        reward_batch = self.reward_buffer[batch]\n",
        "        new_state_batch = self.new_state_buffer[batch]\n",
        "        done_batch = self.terminal_buffer[batch]\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, new_state_batch, done_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q0DxQQj96H0w"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Softmax\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "def DeepQNetwork(lr, num_actions, input_dims, fc1, fc2):\n",
        "    q_net = Sequential()\n",
        "    q_net.add(Dense(fc1, input_dim=input_dims, activation='relu'))\n",
        "    q_net.add(Dense(fc2, activation='relu'))\n",
        "    q_net.add(Dense(num_actions, activation=None))\n",
        "    q_net.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
        "\n",
        "    return q_net\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, lr, discount_factor, num_actions, epsilon, batch_size, input_dims):\n",
        "        self.action_space = [i for i in range(num_actions)]\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon_decay = 0.001\n",
        "        self.epsilon_final = 0.01\n",
        "        self.update_rate = 120\n",
        "        self.step_counter = 0\n",
        "        self.buffer = ReplayBuffer(1000000, input_dims)\n",
        "        self.q_net = DeepQNetwork(lr, num_actions, input_dims, 256, 256)\n",
        "        self.q_target_net = DeepQNetwork(lr, num_actions, input_dims, 256, 256)\n",
        "\n",
        "    def store_tuple(self, state, action, reward, new_state, done):\n",
        "        self.buffer.store_tuples(state, action, reward, new_state, done)\n",
        "\n",
        "    def policy(self, observation):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            state = np.array([observation])\n",
        "            actions = self.q_net(state)\n",
        "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
        "            layer = Softmax()\n",
        "            probs = layer(actions).numpy()\n",
        "        return action, probs\n",
        "\n",
        "    def train(self):\n",
        "        if self.buffer.counter < self.batch_size:\n",
        "            return\n",
        "        if self.step_counter % self.update_rate == 0:\n",
        "            self.q_target_net.set_weights(self.q_net.get_weights())\n",
        "\n",
        "        state_batch, action_batch, reward_batch, new_state_batch, done_batch = \\\n",
        "            self.buffer.sample_buffer(self.batch_size)\n",
        "\n",
        "        q_predicted = self.q_net(state_batch)\n",
        "        q_next = self.q_target_net(new_state_batch)\n",
        "        q_max_next = tf.math.reduce_max(q_next, axis=1, keepdims=True).numpy()\n",
        "        q_target = np.copy(q_predicted)\n",
        "\n",
        "        for idx in range(done_batch.shape[0]):\n",
        "            target_q_val = reward_batch[idx]\n",
        "            if not done_batch[idx]:\n",
        "                target_q_val += self.discount_factor*q_max_next[idx]\n",
        "            q_target[idx, action_batch[idx]] = target_q_val\n",
        "        self.q_net.train_on_batch(state_batch, q_target)\n",
        "        self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.epsilon_final else self.epsilon_final\n",
        "        self.step_counter += 1\n",
        "\n",
        "    def train_model(self, env, num_episodes, graph):\n",
        "\n",
        "        scores, episodes, avg_scores, obj = [], [], [], []\n",
        "        goal = 200\n",
        "        f = 0\n",
        "        txt = open(\"saved_networks.txt\", \"w\")\n",
        "\n",
        "        for i in range(num_episodes):\n",
        "            done = False\n",
        "            score = 0.0\n",
        "            state = env.reset()\n",
        "            while not done:\n",
        "                action = self.policy(state)\n",
        "                new_state, reward, done, _= env.step(action)\n",
        "                score += reward\n",
        "                self.store_tuple(state, action, reward, new_state, done)\n",
        "                state = new_state\n",
        "                self.train()\n",
        "            scores.append(score)\n",
        "            obj.append(goal)\n",
        "            episodes.append(i)\n",
        "            avg_score = np.mean(scores[-100:])\n",
        "            avg_scores.append(avg_score)\n",
        "            print(\"Episode {0}/{1}, Score: {2} ({3}), AVG Score: {4}\".format(i, num_episodes, score, self.epsilon,\n",
        "                                                                             avg_score))\n",
        "            if avg_score >= 170.0 and score >= 200:\n",
        "                self.q_net.save((\"saved_networks/dqn_model{0}\".format(f)))\n",
        "                self.q_net.save_weights((\"saved_networks/dqn_model{0}/net_weights{0}.h5\".format(f)))\n",
        "                txt.write(\"Save {0} - Episode {1}/{2}, Score: {3} ({4}), AVG Score: {5}\\n\".format(f, i, num_episodes,\n",
        "                                                                                                  score, self.epsilon,\n",
        "                                                                                                  avg_score))\n",
        "                f += 1\n",
        "                print(\"Network saved\")\n",
        "\n",
        "        txt.close()\n",
        "        if graph:\n",
        "            df = pd.DataFrame({'x': episodes, 'Score': scores, 'Average Score': avg_scores, 'Solved Requirement': obj})\n",
        "\n",
        "            plt.plot('x', 'Score', data=df, marker='', color='blue', linewidth=2, label='Score')\n",
        "            plt.plot('x', 'Average Score', data=df, marker='', color='orange', linewidth=2, linestyle='dashed',\n",
        "                     label='AverageScore')\n",
        "            plt.plot('x', 'Solved Requirement', data=df, marker='', color='red', linewidth=2, linestyle='dashed',\n",
        "                     label='Solved Requirement')\n",
        "            plt.legend()\n",
        "            plt.savefig('LunarLander_Train.png')\n",
        "\n",
        "    def test(self, env, num_episodes, file_type, file, graph):\n",
        "        self.q_net = tf.keras.models.load_model(file)\n",
        "        self.epsilon = 0.0\n",
        "        scores, episodes, avg_scores, obj = [], [], [], []\n",
        "        goal = 200\n",
        "        score = 0.0\n",
        "        df2 = pd.DataFrame()\n",
        "        for i in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            episode_score = 0.0\n",
        "            k = 1\n",
        "            while not done:\n",
        "                k += 1\n",
        "                #env.render()\n",
        "                action, probs = self.policy(state)\n",
        "                new_state, reward, done, _ = env.step(action)\n",
        "                episode_score += reward\n",
        "                state = new_state\n",
        "                l=[i, k, action]\n",
        "                l.extend(state)\n",
        "                df2 = df2.append(pd.DataFrame([l],columns=[\"episode\",\"ts\", \"action\", \"state1\",\"state2\",\"state3\",\"state4\",\"state5\",\"state6\",\"state7\",\"state8\"]),ignore_index=True)\n",
        "            score += episode_score\n",
        "            scores.append(episode_score)\n",
        "            obj.append(goal)\n",
        "            episodes.append(i)\n",
        "            avg_score = np.mean(scores[-100:])\n",
        "            avg_scores.append(avg_score)\n",
        "\n",
        "        if graph:\n",
        "            df = pd.DataFrame({'x': episodes, 'Score': scores, 'Average Score': avg_scores, 'Solved Requirement': obj})\n",
        "\n",
        "            plt.plot('x', 'Score', data=df, marker='', color='blue', linewidth=2, label='Score')\n",
        "            plt.plot('x', 'Average Score', data=df, marker='', color='orange', linewidth=2, linestyle='dashed',\n",
        "                     label='AverageScore')\n",
        "            plt.plot('x', 'Solved Requirement', data=df, marker='', color='red', linewidth=2, linestyle='dashed',\n",
        "                     label='Solved Requirement')\n",
        "            plt.legend()\n",
        "            plt.savefig('LunarLander_Test.png')\n",
        "        \n",
        "        df2.to_csv('dataset.csv')\n",
        "\n",
        "        with open('expert_traj.pickle', 'wb') as handle:\n",
        "          pickle.dump(df2, handle, protocol=pickle.HIGHEST_PROTOCOL) \n",
        "\n",
        "        env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xJr8FswRKt1P"
      },
      "outputs": [],
      "source": [
        "#!pip install Box2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "4-MFxxL64KSs",
        "outputId": "43a3d0a8-f6a7-4168-db76-ffed5223b75e"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-dd6f9936cfff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-cc42cb3cc748>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, num_episodes, file_type, file, graph)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at saved_networks"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "spec = gym.spec(\"LunarLander-v2\")\n",
        "train = 0\n",
        "test = 1\n",
        "num_episodes = 2\n",
        "graph = True\n",
        "\n",
        "file_type = 'tf'\n",
        "file = 'saved_networks'\n",
        "\n",
        "dqn_agent = Agent(lr=0.00075, discount_factor=0.99, num_actions=4, epsilon=1.0, batch_size=64, input_dims=8)\n",
        "\n",
        "if train and not test:\n",
        "    dqn_agent.train_model(env, num_episodes, graph)\n",
        "else:\n",
        "    dqn_agent.test(env, num_episodes, file_type, file, graph)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "eb312fa740ba79c8bb8bd37f04eb00312f6eaa9c1668818e092a1bc879dc4797"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
